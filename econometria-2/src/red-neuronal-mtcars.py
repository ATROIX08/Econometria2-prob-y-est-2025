# -*- coding: utf-8 -*-
# =============================================================================
# MLP minimalista (pero bien explicado) para clasificar 'cyl' (cilindros) en el
# dataset mtcars (32 filas).
#
# ¬øQu√© aprender√°s leyendo SOLO los COMENTARIOS de este archivo?
# - Por qu√© escalamos los inputs antes de entrenar redes neuronales.
# - C√≥mo separar entrenamiento/prueba sin "hacer trampa" (fuga de informaci√≥n).
# - Qu√© es un Pipeline en scikit-learn y por qu√© es la forma correcta de juntar
#   preprocesamiento + modelo sin fugas.
# - Qu√© es un MLP (perceptr√≥n multicapa), c√≥mo se "arma" con capas, neuronas y
#   funciones de activaci√≥n.
# - Qu√© significan y por qu√© se eligieron los hiperpar√°metros principales:
#   hidden_layer_sizes, activation, solver, alpha (regularizaci√≥n L2),
#   learning_rate, learning_rate_init, max_iter y early_stopping.
# - C√≥mo entrena internamente (muy resumido) un MLPClassifier (backprop + optimizador).
# - C√≥mo evaluar el modelo: accuracy, reporte de clasificaci√≥n, matriz de confusi√≥n,
#   curva de p√©rdida y validaci√≥n cruzada estratificada.
#
# NOTA: Dejamos al final dos visualizaciones opcionales (dibujo de red y heatmap
# de pesos). No son necesarias para entender la red; est√°n como extra. Si no las
# quieres, puedes comentar esas celdas sin afectar lo esencial.
# =============================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from matplotlib.colors import TwoSlopeNorm
import matplotlib.cm as mcm  # <- usamos 'mcm' para evitar colisiones con variables llamadas 'cm'

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ---------- 1) Carga y preparaci√≥n de datos ----------
# Usamos el dataset "mtcars" (cl√°sico de R) desde una URL p√∫blica.
# Contiene varias variables num√©ricas de autos (mpg, hp, wt, etc.) y, entre ellas,
# 'cyl' (n√∫mero de cilindros) que ser√° nuestra variable objetivo (clasificaci√≥n multiclase).
url = "https://gist.githubusercontent.com/seankross/a412dfbd88b3db70b74b/raw/5f23f993cd87c283ce766e7ac6b329ee7cc2e1d1/mtcars.csv"

# Leemos el CSV y renombramos la columna 'Unnamed: 0' a 'model' (nombre del auto).
df = pd.read_csv(url).rename(columns={"Unnamed: 0": "model"})

# Definimos X (features) y y (target):
# - X: TODAS las variables num√©ricas excepto 'model' (texto/etiqueta) y 'cyl' (target).
# - y: la columna 'cyl' convertida a int (por si acaso).
X = df.drop(columns=["model", "cyl"])
y = df["cyl"].astype(int)

# ---------- 2) Split estratificado ----------
# Dividimos en entrenamiento y prueba con estratificaci√≥n:
# - 'test_size=0.3' -> 30% de los datos a prueba.
# - 'stratify=y' asegura que la proporci√≥n de clases (cilindros 4, 6, 8) se mantenga
#   similar en train y test. Esto es crucial en datasets peque√±os para que test sea representativo.
# - 'random_state=42' fija la semilla para reproducibilidad.
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# ---------- 3) Pipeline MLP (n√∫cleo de la red neuronal) ----------
# Un PIPELINE encadena pasos: primero el escalado (StandardScaler), luego el MLP.
# ¬øPor qu√© escalar?
# - El MLP entrena mediante gradiente; si las features tienen escalas muy distintas (hp ~ 100-300,
#   wt ~ 2-5, mpg ~ 10-35, etc.), los gradientes se "desbalancean" y el entrenamiento es ineficiente
#   o inestable. StandardScaler centra las variables (media 0) y las escala a desviaci√≥n est√°ndar 1.
#
# ¬øPor qu√© un Pipeline y no escalar "a mano"?
# - Para evitar "data leakage" (fuga de informaci√≥n). El scaler se ajusta SOLO con X_train y luego
#   se aplica a X_train y X_test. El Pipeline lo garantiza autom√°ticamente y adem√°s se integra
#   perfectamente con la validaci√≥n cruzada (cada fold re-ajusta su scaler dentro del fold).
#
# ¬øQu√© es un MLPClassifier?
# - Es una red neuronal feed-forward b√°sica con:
#   * Capa de entrada: tantas neuronas como features (se infiere de X).
#   * Capas ocultas: definidas por 'hidden_layer_sizes'.
#   * Capa de salida: tantas neuronas como clases (en mtcars, t√≠picamente 3: 4, 6 y 8 cilindros).
# - Para clasificaci√≥n multiclase, la capa de salida usa softmax internamente y entrena minimizando
#   la p√©rdida de entrop√≠a cruzada (log-loss).
#
# Explicaci√≥n de hiperpar√°metros que usamos:
# - hidden_layer_sizes=(32,):
#     Una sola capa oculta con 32 neuronas. M√°s neuronas = m√°s capacidad para modelar relaciones
#     no lineales; pero demasiadas neuronas en datasets peque√±os pueden sobreajustar.
# - activation="relu":
#     Funci√≥n de activaci√≥n ReLU en las capas ocultas. Ayuda con gradientes m√°s estables
#     (evita parte del "vanishing gradient" t√≠pico de sigmoide/tanh) y suele converger m√°s r√°pido.
# - solver="adam":
#     Optimizador Adam (estoc√°stico) que adapta la tasa de aprendizaje por par√°metro usando momentos
#     del gradiente. Es robusto y funciona bien "out of the box". Alternativas:
#       * "lbfgs": optimizador cuasi-Newton, suele ir muy bien en datasets MUY peque√±os,
#         pero no produce 'loss_curve_' ni soporta 'early_stopping'.
#       * "sgd": descenso por gradiente estoc√°stico puro (necesita m√°s tuning).
# - alpha=1e-3:
#     Regularizaci√≥n L2 (tambi√©n llamada "weight decay"). Penaliza pesos grandes para reducir
#     sobreajuste. Si ves overfitting, sube alpha; si el modelo se queda corto, b√°jalo un poco.
# - learning_rate="adaptive" + learning_rate_init=1e-3:
#     Comenzamos con 0.001 y, si el entrenamiento se estanca, Adam puede adaptar internamente
#     la tasa efectiva. Con 'adaptive', MLPClassifier reduce la tasa si no mejora el loss en
#     un n√∫mero de iteraciones consecutivas (cuando no usamos lbfgs).
# - max_iter=2000:
#     L√≠mite m√°ximo de √©pocas/iteraciones de entrenamiento. Con datos peque√±os, a veces el
#     criterio de parada tarda; elevar este l√≠mite permite converger. Si ves "ConvergenceWarning",
#     sube max_iter o ajusta otros hiperpar√°metros.
# - early_stopping=False:
#     Si fuera True, el MLP separa internamente un "validation set" del propio entrenamiento y se
#     detiene cuando deja de mejorar ah√≠. En datasets MUY peque√±os, esto puede quitar demasiados
#     datos al entrenamiento; por eso aqu√≠ lo ponemos en False. (Si tuvieras m√°s datos, probar True
#     suele ser buena idea).
# - random_state=42:
#     Fija inicializaciones (pesos aleatorios, mezcla de datos) para reproducibilidad.
pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("mlp", MLPClassifier(
        hidden_layer_sizes=(32,),    # 1 capa oculta con 32 neuronas (capacidad moderada)
        activation="relu",           # no linealidad en capas ocultas
        solver="adam",               # optimizador robusto y pr√°ctico en general
        alpha=1e-3,                  # regularizaci√≥n L2 (weight decay) para evitar sobreajuste
        learning_rate="adaptive",    # permite reducir la tasa si no hay mejora
        learning_rate_init=1e-3,     # tasa de aprendizaje inicial
        max_iter=2000,               # iteraciones m√°ximas (suficiente para converger en datasets chicos)
        early_stopping=False,        # en datasets peque√±os, mejor no restar muestras de train
        random_state=42              # reproducibilidad (pesos iniciales, orden de muestras)
    ))
])

# ---------- 4) Entrenar ----------
# Aqu√≠ ocurre la "magia": fit() hace dos cosas porque estamos en un Pipeline:
# 1) Ajusta el StandardScaler con X_train (calcula medias y desviaciones) y transforma X_train.
# 2) Con esos datos escalados, entrena la red MLP:
#    - Inicializa pesos de forma aleatoria (controlado por random_state).
#    - Hace forward pass (propagaci√≥n hacia adelante) para obtener predicciones.
#    - Calcula la p√©rdida (log-loss) comparando con y_train (multiclase).
#    - Hace backpropagation (retropropagaci√≥n del error) para calcular gradientes.
#    - Actualiza pesos con Adam (usa promedios m√≥viles de gradientes y gradientes al cuadrado).
#    - Repite (√©pocas) hasta converger o alcanzar max_iter.
pipe.fit(X_train, y_train)

# ---------- 5) Evaluar en test ----------
# Usamos el Pipeline para predecir en X_test. Importante:
# - El scaler dentro del Pipeline usa LOS PAR√ÅMETROS AJUSTADOS EN TRAIN (no se re-ajusta con test).
# - Luego, la red produce la clase con mayor probabilidad (softmax) para cada fila de test.
y_pred = pipe.predict(X_test)

# Accuracy: proporci√≥n de aciertos (r√°pido de interpretar, pero ojo con clases desbalanceadas).
acc = accuracy_score(y_test, y_pred)
print(f"Accuracy (test): {acc:.3f}\n")

# Reporte de clasificaci√≥n: precisi√≥n, recall, F1 por clase y promedios.
# - precision: de lo que predije como "clase k", ¬øqu√© % fue correcto?
# - recall (sensibilidad): de los "verdaderos clase k", ¬øqu√© % detect√©?
# - F1: media arm√≥nica de precision y recall (balancea ambos).
print("Classification report (test):\n",
      classification_report(y_test, y_pred, zero_division=0,
                            target_names=[f"{c} cylinders" for c in sorted(y.unique())]))

# ---------- 6) Matriz de confusi√≥n ----------
# La matriz de confusi√≥n muestra conteos de (clase real vs predicha).
# Diagonal alta = buen desempe√±o. Fuera de diagonal = confusiones.
labels = sorted(y.unique())
conf_mat = confusion_matrix(y_test, y_pred, labels=labels)  # <- evitamos usar el nombre 'cm' por claridad
plt.figure(figsize=(5, 4))
sns.heatmap(conf_mat, annot=True, fmt="d", cmap="Blues",
            xticklabels=labels, yticklabels=labels)
plt.title("Matriz de confusi√≥n (conteos)")
plt.xlabel("Predicci√≥n"); plt.ylabel("Real")
plt.tight_layout()
plt.show()

# ---------- 7) Curva de p√©rdida (c√≥mo baja el error durante el entrenamiento) ----------
# El MLP almacena 'loss_curve_' (p√©rdida por iteraci√≥n) solo con solvers que iteran (adam/sgd).
# Esto ayuda a diagnosticar si:
# - La p√©rdida baja y se estabiliza (bien).
# - La p√©rdida oscila o no baja (tal vez LR es grande o faltan √©pocas).
# - La p√©rdida baja muy lento (quiz√° LR muy chico).
mlp_trained = pipe.named_steps["mlp"]
if hasattr(mlp_trained, "loss_curve_"):
    plt.figure(figsize=(6, 4))
    plt.plot(mlp_trained.loss_curve_)
    plt.xlabel("Iteraciones / √âpocas")
    plt.ylabel("Loss (log-loss)")
    plt.title("Curva de p√©rdida del MLP")
    plt.tight_layout()
    plt.show()

# ---------- 8) (Opcional) Validaci√≥n cruzada estratificada 5-fold ----------
# Para tener una m√©trica m√°s "estable" con tan pocos datos, usamos CV:
# - StratifiedKFold preserva proporciones de clase en cada fold.
# - cross_val_score re-ajusta el Pipeline en cada fold (¬°sin fugas!), devolviendo accuracy por fold.
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_acc = cross_val_score(pipe, X, y, cv=cv, scoring="accuracy")
print(f"Accuracy CV 5-fold (media ¬± sd): {cv_acc.mean():.3f} ¬± {cv_acc.std():.3f}")

# =============================================================================
# ======================   PLOT DE LA RED (OPCIONAL)   ========================
# Las siguientes dos secciones son VISUALIZACIONES OPCIONALES para:
# (A) Dibujar la arquitectura de la red con conexiones coloreadas por peso.
# (B) Ver un heatmap de los pesos de entrada ‚Üí capa oculta.
# Puedes comentarlas si solo te interesa el entrenamiento/evaluaci√≥n.
# =============================================================================


def plot_mlp_network(pipe, feature_names=None, class_names=None,
                     edge_maxwidth=4.0, weight_threshold=0.05, figsize=(11, 6), dpi=200):
    """
    Dibuja la arquitectura y pesos del MLP entrenado dentro de un Pipeline.

    ¬øQu√© muestra?
    - Nodos por capa (entrada, ocultas, salida).
    - Aristas (conexiones) coloreadas por signo del peso y con grosor proporcional
      a su magnitud. As√≠ se intuye qu√© features/caminos pesan m√°s.

    Par√°metros:
    - edge_maxwidth: grosor m√°ximo de arista (se escala por |peso|).
    - weight_threshold: porcentaje del peso m√°ximo por capa para filtrar conexiones
      MUY d√©biles (reduce ruido visual).
    - figsize: tama√±o del gr√°fico.
    - dpi: resoluci√≥n para una visualizaci√≥n m√°s n√≠tida.

    Nota: Esto usa los atributos 'coefs_' del MLP (lista de matrices de pesos W^(l)).
    Cada W tiene forma [n_l, n_{l+1}], es decir, pesos de capa L a capa L+1.
    """
    # Extraemos el MLP desde el Pipeline y sus matrices de pesos.
    mlp = pipe.named_steps["mlp"]
    coefs = mlp.coefs_
    # intercepts = mlp.intercepts_  # existen, pero aqu√≠ no se grafican.

    # Calculamos tama√±os de cada capa:
    # - La capa de entrada tiene tantas neuronas como columnas de X.
    # - Luego, por cada matriz de pesos W, su dimensi√≥n .shape[1] es el n√∫mero de neuronas en la capa siguiente.
    n_layers = len(coefs) + 1
    layer_sizes = [coefs[0].shape[0]] + [W.shape[1] for W in coefs]  # [n_in, ..., n_out]

    # Nombres default si no se pasan:
    if feature_names is None:
        feature_names = [f"x{i+1}" for i in range(layer_sizes[0])]
    if class_names is None:
        class_names = [str(c) for c in mlp.classes_]

    # Posiciones (x, y) de nodos por capa para dibujar:
    xs = np.linspace(0, n_layers - 1, n_layers)
    positions = []
    for L, size in enumerate(layer_sizes):
        # Distribuci√≥n vertical uniforme de nodos dentro de [0, 1] para esa capa:
        ys = np.linspace(0, 1, size)
        positions.append(list(zip([xs[L]] * size, ys)))

    # Preparar normalizaci√≥n de color en torno a 0 (negativos a un color, positivos a otro).
    allW = np.concatenate([W.ravel() for W in coefs]) if len(coefs) else np.array([0.0])
    vmax = np.max(np.abs(allW)) if np.max(np.abs(allW)) > 0 else 1.0
    norm = TwoSlopeNorm(vmin=-vmax, vcenter=0.0, vmax=vmax)
    cmap = mcm.get_cmap("coolwarm")

    # üîß Cambiamos la creaci√≥n de la figura para poder controlar DPI y m√°rgenes
    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)
    ax.set_title("Red neuronal (MLP): nodos y pesos aprendidos")
    ax.axis("off")

    # Deja un peque√±o margen a la derecha para que las etiquetas de salida no queden pegadas.
    # (Adem√°s, como la barra de color ir√° en un eje separado, este margen ayuda a que nada se corte).
    ax.set_xlim(-0.2, (n_layers - 1) + 0.25)

    # Dibujar aristas (conexiones) capa a capa:
    for L, W in enumerate(coefs):
        absW = np.abs(W)
        thr = weight_threshold * absW.max() if absW.size else 0.0
        for i in range(W.shape[0]):       # neurona i en capa L
            x0, y0 = positions[L][i]
            for j in range(W.shape[1]):   # neurona j en capa L+1
                w = W[i, j]
                if np.abs(w) < thr:
                    continue  # filtra pesos muy peque√±os para evitar "spaghetti"
                x1, y1 = positions[L + 1][j]
                # Grosor proporcional a |w| y un m√≠nimo para que se vea:
                lw = max(0.3, edge_maxwidth * (np.abs(w) / vmax))
                ax.plot([x0, x1], [y0, y1],
                        color=cmap(norm(w)), linewidth=lw, alpha=0.8)

    # Dibujar nodos de cada capa:
    for L, coords in enumerate(positions):
        xsL, ysL = zip(*coords)
        ax.scatter(xsL, ysL, s=250, c="#f0f0f0", edgecolors="#444", zorder=3)
        # Etiquetas para capa de entrada (features):
        if L == 0:
            for idx, (x, y_) in enumerate(coords):
                ax.text(x - 0.05, y_, feature_names[idx], ha="right", va="center", fontsize=8)
        # Etiquetas para capa de salida (clases):
        if L == n_layers - 1:
            for idx, (x, y_) in enumerate(coords):
                ax.text(x + 0.05, y_, class_names[idx], ha="left", va="center",
                        fontsize=9, fontweight="bold")

     # -------------------------------------------------------------------------
    from mpl_toolkits.axes_grid1 import make_axes_locatable
    divider = make_axes_locatable(ax)
    # 'size' controla el ancho de la barra; 'pad' la separaci√≥n respecto al eje principal.
    cax = divider.append_axes("right", size="3%", pad=0.5)

    sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)
    cbar = fig.colorbar(sm, cax=cax)
    cbar.set_label("Peso (signo y magnitud)")

    # Ajuste final para que nada se corte (respeta el eje extra del colorbar).
    fig.tight_layout()
    plt.show()

# ---- Llamada del diagrama de la red (opcional) ----
feature_names = X.columns.tolist()
class_names = [f"{int(c)} cylinders" for c in sorted(y.unique())]
plot_mlp_network(pipe, feature_names=feature_names, class_names=class_names,
                 edge_maxwidth=4.0, weight_threshold=0.05, figsize=(11.5, 6.5), dpi=200)

# ---- Heatmap de pesos entrada ‚Üí capa oculta (opcional) ----
# Visualizaci√≥n complementaria: muestra, en una matriz, qu√© tan fuerte (y con qu√© signo)
# conecta cada feature con cada neurona de la capa oculta. √ötil para interpretar tendencias:
# - Pesos positivos grandes: esa feature "empuja" la activaci√≥n de esa neurona hacia arriba.
# - Pesos negativos grandes: la "frenan".
W_in = pipe.named_steps["mlp"].coefs_[0]  # matriz [n_features, n_hidden]
dfW = pd.DataFrame(W_in, index=feature_names,
                   columns=[f"h{j+1}" for j in range(W_in.shape[1])])

plt.figure(figsize=(min(12, 1.2 * W_in.shape[1]), 6))
sns.heatmap(dfW, cmap="coolwarm", center=0, robust=True)
plt.title("Pesos entrada ‚Üí capa oculta (signo y magnitud)")
plt.xlabel("Neuronas ocultas"); plt.ylabel("Features")
plt.tight_layout(); plt.show()

# =============================================================================
# SUGERENCIAS PR√ÅCTICAS (para que sepas "qu√© tocar" si algo no converge o no rinde):
#
# - Si ves "ConvergenceWarning" (no converge):
#     * Sube max_iter (p. ej., 5000) o
#     * Baja learning_rate_init (p. ej., 5e-4) o
#     * Cambia solver a "lbfgs" (en datasets muy chicos puede converger m√°s r√°pido).
#
# - Si rinde bien en train pero mal en test (overfitting):
#     * Sube alpha (p. ej., 1e-2 o 1e-1).
#     * Reduce hidden_layer_sizes (menos neuronas o menos capas).
#     * Usa early_stopping=True si tienes m√°s datos (separa un validation interno).
#
# - Si rinde mal en train y test (underfitting):
#     * Baja alpha (p. ej., 1e-4).
#     * Aumenta hidden_layer_sizes (m√°s neuronas/capas).
#     * Cambia activation a 'tanh' (a veces captura relaciones distintas).
#
# - Para comparar con otros modelos:
#     * Prueba 'lbfgs' como solver (r√°pido en datasets peque√±os, pero sin loss_curve_).
#     * Prueba modelos no neurales (LogisticRegression/RandomForest) como baseline.
#
# Recuerda: este dataset es muy peque√±o (32 filas). Los resultados pueden variar con splits distintos.
# =============================================================================
